<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Absorption and Reward | Lecture notes for “Introduction to Stochastic Processes”</title>
  <meta name="description" content="A set of lecture notes for M362M: Introduction to Stochastic Processes" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Absorption and Reward | Lecture notes for “Introduction to Stochastic Processes”" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A set of lecture notes for M362M: Introduction to Stochastic Processes" />
  <meta name="github-repo" content="gordanz/M362M" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Absorption and Reward | Lecture notes for “Introduction to Stochastic Processes”" />
  
  <meta name="twitter:description" content="A set of lecture notes for M362M: Introduction to Stochastic Processes" />
  

<meta name="author" content="Gordan Zitkovic" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification-of-states.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">M362M Lecture notes</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> An intro to R and RStudio</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#setting-up-an-r-environment-on-your-computer"><i class="fa fa-check"></i><b>1.1</b> Setting up an R environment on your computer</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#installing-r"><i class="fa fa-check"></i><b>1.1.1</b> Installing R</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#installing-rstudio"><i class="fa fa-check"></i><b>1.1.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#installing-basic-packages"><i class="fa fa-check"></i><b>1.1.3</b> Installing basic packages</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#learning-the-basics-of-r"><i class="fa fa-check"></i><b>1.2</b> Learning the basics of R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#the-console-scripts-and-r-notebooks"><i class="fa fa-check"></i><b>1.2.1</b> The console, Scripts and R Notebooks</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#asking-for-help"><i class="fa fa-check"></i><b>1.2.2</b> Asking for help</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro.html"><a href="intro.html#vectors"><i class="fa fa-check"></i><b>1.2.3</b> Vectors</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro.html"><a href="intro.html#matrices"><i class="fa fa-check"></i><b>1.2.4</b> Matrices</a></li>
<li class="chapter" data-level="1.2.5" data-path="intro.html"><a href="intro.html#functions"><i class="fa fa-check"></i><b>1.2.5</b> Functions</a></li>
<li class="chapter" data-level="1.2.6" data-path="intro.html"><a href="intro.html#if-else-statements"><i class="fa fa-check"></i><b>1.2.6</b> If-else statements</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#additional-problems-for-chapter-1"><i class="fa fa-check"></i><b>1.3</b> Additional Problems for Chapter 1</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#endnotes"><i class="fa fa-check"></i><b>1.4</b> Endnotes</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html"><i class="fa fa-check"></i><b>2</b> Simulation of Random Variables and Monte Carlo</a>
<ul>
<li class="chapter" data-level="2.1" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html#simulation-of-some-common-probability-distributions"><i class="fa fa-check"></i><b>2.1</b> Simulation of some common probability distributions</a></li>
<li class="chapter" data-level="2.2" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html#multivariate-distributions"><i class="fa fa-check"></i><b>2.2</b> Multivariate Distributions</a></li>
<li class="chapter" data-level="2.3" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html#monte-carlo"><i class="fa fa-check"></i><b>2.3</b> Monte Carlo</a></li>
<li class="chapter" data-level="2.4" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html#conditional-distributions"><i class="fa fa-check"></i><b>2.4</b> Conditional distributions</a></li>
<li class="chapter" data-level="2.5" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html#additional-problems-for-chapter-2"><i class="fa fa-check"></i><b>2.5</b> Additional Problems for Chapter 2</a></li>
<li class="chapter" data-level="2.6" data-path="simulation-of-random-variables-and-monte-carlo.html"><a href="simulation-of-random-variables-and-monte-carlo.html#endnotes-1"><i class="fa fa-check"></i><b>2.6</b> Endnotes</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="random-walks.html"><a href="random-walks.html"><i class="fa fa-check"></i><b>3</b> Random Walks</a>
<ul>
<li class="chapter" data-level="3.1" data-path="random-walks.html"><a href="random-walks.html#what-are-stochastic-processes"><i class="fa fa-check"></i><b>3.1</b> What are stochastic processes?</a></li>
<li class="chapter" data-level="3.2" data-path="random-walks.html"><a href="random-walks.html#the-simple-symmetric-random-walk"><i class="fa fa-check"></i><b>3.2</b> The Simple Symmetric Random Walk</a></li>
<li class="chapter" data-level="3.3" data-path="random-walks.html"><a href="random-walks.html#how-to-simulate-random-walks"><i class="fa fa-check"></i><b>3.3</b> How to simulate random walks</a></li>
<li class="chapter" data-level="3.4" data-path="random-walks.html"><a href="random-walks.html#two-ways-of-looking-at-a-stochastic-proceses"><i class="fa fa-check"></i><b>3.4</b> Two ways of looking at a stochastic proceses</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="random-walks.html"><a href="random-walks.html#column-wise-distributionally"><i class="fa fa-check"></i><b>3.4.1</b> Column-wise (distributionally)</a></li>
<li class="chapter" data-level="3.4.2" data-path="random-walks.html"><a href="random-walks.html#row-wise-trajectorially-or-path-wise"><i class="fa fa-check"></i><b>3.4.2</b> Row-wise (trajectorially or path-wise)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="random-walks.html"><a href="random-walks.html#the-path-space"><i class="fa fa-check"></i><b>3.5</b> The path space</a></li>
<li class="chapter" data-level="3.6" data-path="random-walks.html"><a href="random-walks.html#the-distribution-of-x_n"><i class="fa fa-check"></i><b>3.6</b> The distribution of <span class="math inline">\(X_n\)</span></a></li>
<li class="chapter" data-level="3.7" data-path="random-walks.html"><a href="random-walks.html#biased-random-walks"><i class="fa fa-check"></i><b>3.7</b> Biased random walks</a></li>
<li class="chapter" data-level="3.8" data-path="random-walks.html"><a href="random-walks.html#additional-problems-for-chapter-3"><i class="fa fa-check"></i><b>3.8</b> Additional problems for Chapter 3</a></li>
<li class="chapter" data-level="3.9" data-path="random-walks.html"><a href="random-walks.html#endnotes-2"><i class="fa fa-check"></i><b>3.9</b> Endnotes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="more-about-random-walks.html"><a href="more-about-random-walks.html"><i class="fa fa-check"></i><b>4</b> More about Random Walks</a>
<ul>
<li class="chapter" data-level="4.1" data-path="more-about-random-walks.html"><a href="more-about-random-walks.html#the-reflection-principle"><i class="fa fa-check"></i><b>4.1</b> The reflection principle</a></li>
<li class="chapter" data-level="4.2" data-path="more-about-random-walks.html"><a href="more-about-random-walks.html#stopping-times"><i class="fa fa-check"></i><b>4.2</b> Stopping times</a></li>
<li class="chapter" data-level="4.3" data-path="more-about-random-walks.html"><a href="more-about-random-walks.html#walds-identity-and-gamblers-ruin"><i class="fa fa-check"></i><b>4.3</b> Wald’s identity and Gambler’s ruin</a></li>
<li class="chapter" data-level="4.4" data-path="more-about-random-walks.html"><a href="more-about-random-walks.html#additional-problems-for-chapter-4"><i class="fa fa-check"></i><b>4.4</b> Additional problems for Chapter 4</a></li>
<li class="chapter" data-level="4.5" data-path="more-about-random-walks.html"><a href="more-about-random-walks.html#endnotes-3"><i class="fa fa-check"></i><b>4.5</b> Endnotes</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="markov-chains.html"><a href="markov-chains.html"><i class="fa fa-check"></i><b>5</b> Markov Chains</a>
<ul>
<li class="chapter" data-level="5.1" data-path="markov-chains.html"><a href="markov-chains.html#the-markov-property"><i class="fa fa-check"></i><b>5.1</b> The Markov property</a></li>
<li class="chapter" data-level="5.2" data-path="markov-chains.html"><a href="markov-chains.html#first-examples"><i class="fa fa-check"></i><b>5.2</b> First Examples</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="markov-chains.html"><a href="markov-chains.html#random-walks-1"><i class="fa fa-check"></i><b>5.2.1</b> Random walks</a></li>
<li class="chapter" data-level="5.2.2" data-path="markov-chains.html"><a href="markov-chains.html#gambler"><i class="fa fa-check"></i><b>5.2.2</b> Gambler’s ruin</a></li>
<li class="chapter" data-level="5.2.3" data-path="markov-chains.html"><a href="markov-chains.html#regime-switching"><i class="fa fa-check"></i><b>5.2.3</b> Regime Switching</a></li>
<li class="chapter" data-level="5.2.4" data-path="markov-chains.html"><a href="markov-chains.html#deterministically-monotone-markov-chain"><i class="fa fa-check"></i><b>5.2.4</b> Deterministically monotone Markov chain</a></li>
<li class="chapter" data-level="5.2.5" data-path="markov-chains.html"><a href="markov-chains.html#not-a-markov-chain"><i class="fa fa-check"></i><b>5.2.5</b> Not a Markov chain</a></li>
<li class="chapter" data-level="5.2.6" data-path="markov-chains.html"><a href="markov-chains.html#turning-a-non-markov-chain-into-a-markov-chain"><i class="fa fa-check"></i><b>5.2.6</b> Turning a non-Markov chain into a Markov chain</a></li>
<li class="chapter" data-level="5.2.7" data-path="markov-chains.html"><a href="markov-chains.html#deterministic-functions-of-markov-chains-do-not-need-to-be-markov-chains"><i class="fa fa-check"></i><b>5.2.7</b> Deterministic functions of Markov chains do not need to be Markov chains</a></li>
<li class="chapter" data-level="5.2.8" data-path="markov-chains.html"><a href="markov-chains.html#a-game-of-tennis"><i class="fa fa-check"></i><b>5.2.8</b> A game of tennis</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="markov-chains.html"><a href="markov-chains.html#chapman-kolmogorov-equations"><i class="fa fa-check"></i><b>5.3</b> Chapman-Kolmogorov equations</a></li>
<li class="chapter" data-level="5.4" data-path="markov-chains.html"><a href="markov-chains.html#mc-sim"><i class="fa fa-check"></i><b>5.4</b> How to simulate Markov chains</a></li>
<li class="chapter" data-level="5.5" data-path="markov-chains.html"><a href="markov-chains.html#additional-problems-for-chapter-5"><i class="fa fa-check"></i><b>5.5</b> Additional problems for Chapter 5</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification-of-states.html"><a href="classification-of-states.html"><i class="fa fa-check"></i><b>6</b> Classification of States</a>
<ul>
<li class="chapter" data-level="6.1" data-path="classification-of-states.html"><a href="classification-of-states.html#the-communication-relation"><i class="fa fa-check"></i><b>6.1</b> The Communication Relation</a></li>
<li class="chapter" data-level="6.2" data-path="classification-of-states.html"><a href="classification-of-states.html#classes"><i class="fa fa-check"></i><b>6.2</b> Classes</a></li>
<li class="chapter" data-level="6.3" data-path="classification-of-states.html"><a href="classification-of-states.html#transience-and-recurrence"><i class="fa fa-check"></i><b>6.3</b> Transience and recurrence</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="classification-of-states.html"><a href="classification-of-states.html#the-return-theorem"><i class="fa fa-check"></i><b>6.3.1</b> The Return Theorem</a></li>
<li class="chapter" data-level="6.3.2" data-path="classification-of-states.html"><a href="classification-of-states.html#a-recurrence-criterion"><i class="fa fa-check"></i><b>6.3.2</b> A recurrence criterion</a></li>
<li class="chapter" data-level="6.3.3" data-path="classification-of-states.html"><a href="classification-of-states.html#polyas-theorem"><i class="fa fa-check"></i><b>6.3.3</b> Polya’s theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="classification-of-states.html"><a href="classification-of-states.html#class-properties"><i class="fa fa-check"></i><b>6.4</b> Class properties</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="classification-of-states.html"><a href="classification-of-states.html#the-canonical-decomposition"><i class="fa fa-check"></i><b>6.4.1</b> The Canonical Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="classification-of-states.html"><a href="classification-of-states.html#a-few-examples"><i class="fa fa-check"></i><b>6.5</b> A few examples</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="classification-of-states.html"><a href="classification-of-states.html#random-walks-2"><i class="fa fa-check"></i><b>6.5.1</b> Random walks</a></li>
<li class="chapter" data-level="6.5.2" data-path="classification-of-states.html"><a href="classification-of-states.html#gamblers-ruin"><i class="fa fa-check"></i><b>6.5.2</b> Gambler’s ruin</a></li>
<li class="chapter" data-level="6.5.3" data-path="classification-of-states.html"><a href="classification-of-states.html#deterministically-monotone-markov-chain-1"><i class="fa fa-check"></i><b>6.5.3</b> Deterministically monotone Markov chain</a></li>
<li class="chapter" data-level="6.5.4" data-path="classification-of-states.html"><a href="classification-of-states.html#the-game-of-tennis"><i class="fa fa-check"></i><b>6.5.4</b> The game of tennis</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="classification-of-states.html"><a href="classification-of-states.html#additional-problems-for-chapter-6"><i class="fa fa-check"></i><b>6.6</b> Additional problems for Chapter 6</a></li>
<li class="chapter" data-level="6.7" data-path="classification-of-states.html"><a href="classification-of-states.html#endnotes-4"><i class="fa fa-check"></i><b>6.7</b> Endnotes</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="absorption-and-reward.html"><a href="absorption-and-reward.html"><i class="fa fa-check"></i><b>7</b> Absorption and Reward</a>
<ul>
<li class="chapter" data-level="7.1" data-path="absorption-and-reward.html"><a href="absorption-and-reward.html#absorption"><i class="fa fa-check"></i><b>7.1</b> Absorption</a></li>
<li class="chapter" data-level="7.2" data-path="absorption-and-reward.html"><a href="absorption-and-reward.html#expected-reward"><i class="fa fa-check"></i><b>7.2</b> Expected reward</a></li>
<li class="chapter" data-level="7.3" data-path="absorption-and-reward.html"><a href="absorption-and-reward.html#additional-problems-for-chapter-7"><i class="fa fa-check"></i><b>7.3</b> Additional Problems for Chapter 7</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for “Introduction to Stochastic Processes”</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="absorption-and-reward" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Absorption and Reward<a href="absorption-and-reward.html#absorption-and-reward" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="counter-reset: thechapter 7;">

</div>
<p><strong>Caveat:</strong> From now on, all Markov chains will have <strong>finite</strong> state spaces.</p>
<div id="absorption" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Absorption<a href="absorption-and-reward.html#absorption" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Remember the “Tennis” example from a few lectures ago and the question
we asked there, namely, how does the probability of winning a single
point affect the probability of winning the overall game? An algorithm
that will help you answer that question will be described in this
lecture.</p>
<p>The first step is to understand the structure of the question asked in
the light of the canonical decomposition of the previous lecture. In the
“Tennis” example, all the states except for the winning ones are
transient, and there are two one-element recurrent classes {“Player 1 wins”}
and {“Player 2 wins”} The chain starts from a transient
state <span class="math inline">\((0,0)\)</span>, moves around a bit, and, eventually, gets absorbed in one
of the two. The probability we are interested in is not the probability
that the chain will eventually get absorbed. That probability is always
<span class="math inline">\(1\)</span>. We are, instead, interested in the probability that the absorption
will occur in a particular state - the state “Player 1 wins” (as
opposed to “Player 2 wins”) in the “Tennis” example.</p>
<p>A more general version of the problem above is the following: let
<span class="math inline">\(i\in S\)</span> be any state, and let <span class="math inline">\(j\)</span> be a recurrent state. If the set of
all recurrent states is denoted by <span class="math inline">\(C\)</span>, and if <span class="math inline">\(\tau_{C}\)</span> is the first
hitting time of the set <span class="math inline">\(C\)</span>, then <span class="math inline">\(X_{\tau_{C}}\)</span> denotes the first
recurrent state visited by the chain. Equivalently, <span class="math inline">\(X_{\tau_{C}}\)</span> is
the value of <span class="math inline">\(X\)</span> at (random) time <span class="math inline">\(\tau_{C}\)</span>; its value is the name of
the state in which it happens to find itself the first time it hits the
set of all recurrent states. For any two states <span class="math inline">\(i,j\in S\)</span>, the <span class="math inline">\(u_{ij}\)</span>
is defined as
<span class="math display">\[u_{ij}={\mathbb{P}}_i[ X_{\tau_C}=j]={\mathbb{P}}_i[\text{ the first recurrent state visited by $X$ is $j$ }].\]</span> There are several boring situations to
discard first:</p>
<ol style="list-style-type: decimal">
<li><p><em><span class="math inline">\(j\)</span> is transient:</em> in this case <span class="math inline">\(u_{ij}=0\)</span> for any <span class="math inline">\(i\)</span> because <span class="math inline">\(j\)</span>
cannot possibly be the first recurrent state we hit - it is not even
recurrent.</p></li>
<li><p><em><span class="math inline">\(j\)</span> is recurrent, and so is <span class="math inline">\(i\)</span></em>. Since <span class="math inline">\(i\)</span> is recurrent, i.e.,
<span class="math inline">\(i\in C\)</span>, we clearly have <span class="math inline">\(\tau_C=0\)</span>. Therefore <span class="math inline">\(u_{ij} = {\mathbb{P}}_i[  X_0= j]\)</span>, and this equals to either <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>, depending on
whether <span class="math inline">\(i=j\)</span> or <span class="math inline">\(i\ne j\)</span>.</p></li>
</ol>
<p>That leaves us with the situation where <span class="math inline">\(i \in T\)</span> and <span class="math inline">\(j\in C\)</span> as the
interesting one. In many calculations related to Markov chains, the
method of <em>first-step decomposition</em> works miracles. Simply, we cut the
probability space according to what happened in the first step and use
the law of total probability (assuming <span class="math inline">\(i\in T\)</span>, <span class="math inline">\(j\in C\)</span>)
<span class="math display">\[\label{equ:system-for-u}
   \nonumber
   \begin{split}
u_{ij} &amp; ={\mathbb{P}}_i[ X_{\tau_C}=j]=\sum_{k\in S}
{\mathbb{P}}[X_{\tau_C}=j|X_0=i, X_1=k] {\mathbb{P}}[ X_1=k|X_0=i]\\
&amp;=
\sum_{k\in S}
{\mathbb{P}}[X_{\tau_C}=j|X_1=k]p_{ik}
   \end{split}\]</span> The conditional probability <span class="math inline">\({\mathbb{P}}[X_{\tau_C}=j|X_1=k]\)</span>
is an absorption probability, too. If <span class="math inline">\(k=j\)</span>, then
<span class="math inline">\({\mathbb{P}}[X_{\tau_C}=j|X_1=k]=1\)</span>. If <span class="math inline">\(k\in C\setminus\{j\}\)</span>, then we are
already in C, but in a state different from <span class="math inline">\(j\)</span>, so <span class="math inline">\({\mathbb{P}}[ X_{\tau_C}=j|X_1=k]=0\)</span>. Therefore, the sum above can be written as
<span class="math display">\[\label{equ:syst}
\begin{split}
   u_{ij}= \sum_{k\in T} p_{ik} u_{kj} + p_{ij},
\end{split}\]</span> which is a system of linear equations for the family
<span class="math inline">\(( u_{ij}, i\in T, j\in C)\)</span>. Linear systems are typically better understood when
represented in the matrix form. Let <span class="math inline">\(U\)</span> be a <span class="math inline">\(T\times C\)</span>-matrix
<span class="math inline">\(U=(u_{ij}, i\in T, j\in C)\)</span>, and let <span class="math inline">\(Q\)</span> be the portion of the
transition matrix <span class="math inline">\(P\)</span> corresponding to the transitions from <span class="math inline">\(T\)</span> to <span class="math inline">\(T\)</span>,
i.e. <span class="math inline">\(Q=(p_{ij},i\in T, j\in T)\)</span>, and let <span class="math inline">\(R\)</span> contain all transitions
from <span class="math inline">\(T\)</span> to <span class="math inline">\(C\)</span>, i.e., <span class="math inline">\(R=(p_{ij})_{i\in T, j\in C}\)</span>. If <span class="math inline">\(P_C\)</span> denotes
the matrix of all transitions from <span class="math inline">\(C\)</span> to <span class="math inline">\(C\)</span>, i.e.,
<span class="math inline">\(P_C=(p_{ij}, i\in C, j\in C)\)</span>, then the canonical form of <span class="math inline">\(P\)</span> looks
like this: <span class="math display">\[P=
\begin{bmatrix}
P_C &amp; 0 \\ R &amp; Q
\end{bmatrix}.\]</span> The system
now becomes: <span class="math display">\[U= QU+R,\text{ i.e., } (I-Q) U = R.\]</span> If the matrix <span class="math inline">\(I-Q\)</span>
happens to be invertible, we are in business, because we then have an
explicit expression for <span class="math inline">\(U\)</span>: <span class="math display">\[U= (I-Q)^{-1} R.\]</span> So, is <span class="math inline">\(I-Q\)</span>
invertible? It is when the state space <span class="math inline">\(S\)</span> is finite; here is the
argument, in case you are interested:</p>
<p><strong>Theorem.</strong> When the state space <span class="math inline">\(S\)</span> is finite, the
matrix <span class="math inline">\(I-Q\)</span> is invertible and <span class="math display">\[
\begin{split}
   (I-Q)^{-1} = \sum_{n=0}^{\infty} Q^n.
\end{split}\]</span> Moreover, the entry at the position <span class="math inline">\(i,j\)</span> in <span class="math inline">\((I-Q)^{-1}\)</span>
is the expected total number of visits to the state <span class="math inline">\(j\)</span>, for a chain
started at <span class="math inline">\(i\)</span>.</p>
<p><strong>Proof.</strong> For <span class="math inline">\(k\in{\mathbb{N}}\)</span>, the matrix <span class="math inline">\(Q^k\)</span> is the same as the submatrix
corresponding to the transient states of the full <span class="math inline">\(k\)</span>-step transition
matrix <span class="math inline">\(P^k\)</span>. Indeed, going from a transient state to another transient
state in <span class="math inline">\(k\)</span> steps can only happen via other transient states (once we
hit a recurrent class, we are stuck there forever).</p>
<p>Using the same idea as in the proof of our recurrence criterion in the previous chapter
we can conclude that for any two transient states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>,
we have (remember <span class="math inline">\({\mathbb{E}}_i[  \mathbf{1}_{\{X_n = j\}}] = {\mathbb{P}}_i[X_n = j] = p_{ij}^{(n)}\)</span>)
<span class="math display">\[{\mathbb{E}}_i[ \sum_{n=0}^{\infty} \mathbf{1}_{\{X_n = j\}}] = \sum_{n\in{\mathbb{N}_0}} p^{(n)}_{ij} =
  \sum_{n\in{\mathbb{N}_0}} q^{(n)}_{ij} = (\sum_{n\in{\mathbb{N}}_0} Q^n)_{ij}.\]</span> On the
other hand, the left hand side above is simply the expected number of
visits to the state <span class="math inline">\(j\)</span>, if we start from <span class="math inline">\(i\)</span>. Since both <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>
are transient, this number will either be <span class="math inline">\(0\)</span> (if the chain never even
reaches <span class="math inline">\(j\)</span> from <span class="math inline">\(i\)</span>), or a geometric random variable (if it does). In
either case, the expected value of this quantity is finite, and, so
<span class="math display">\[\sum_{n\in{\mathbb{N}}_0} q^{(n)}_{ij}&lt;\infty.\]</span> Therefore, the matrix sum
<span class="math inline">\(F = \sum_{n\in{\mathbb{N}}_0} Q^n\)</span> is well defined, and it remains to make sure
that <span class="math inline">\(F = (I-Q)^{-1}\)</span>, which follows from the following simple
computation:
<span class="math display">\[QF = Q \sum_{n\in{\mathbb{N}}_0} Q^n = \sum_{n\in{\mathbb{N}}_0} Q^{n+1} = \sum_{n\in{\mathbb{N}}} Q^n =
  \sum_{n\in{\mathbb{N}}_0} Q^n - I = F - I. \text{ Q.E.D.}\]</span></p>
<p>When the inverse <span class="math inline">\((I-Q)^{-1}\)</span> exists (like in the finite case), it is
called the <strong>fundamental matrix</strong> of the Markov chain.</p>
<p>Before we turn to the “Tennis” example, let us analyze a simpler case of
Gambler’s ruin with <span class="math inline">\(a=3\)</span>.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-194" class="exercise"><strong>Problem 7.1  </strong></span>What is the probability that a gambler coming in at <span class="math inline">\(x=\$1\)</span> in a Gambler’s ruin problem with <span class="math inline">\(a=3\)</span> succeeds in “getting rich”? We <em>do not</em> assume that <span class="math inline">\(p=\tfrac{1}{2}\)</span>.</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-195" class="solution"><em>Solution</em>. </span>The states <span class="math inline">\(0\)</span> and <span class="math inline">\(3\)</span> are absorbing, and all
the others are transient. Therefore <span class="math inline">\(C_1=\{0\}\)</span>, <span class="math inline">\(C_2=\{3\}\)</span> and
<span class="math inline">\(T=T_1=\{1,2\}\)</span>. The transition matrix <span class="math inline">\(P\)</span> in the canonical form (the
rows and columns represent the states in the order <span class="math inline">\(0,3,1,2\)</span>) <span class="math display">\[P=
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
1-p &amp; 0 &amp; 0 &amp; p\\
0 &amp; p &amp; 1-p &amp; 0
\end{bmatrix}\]</span> Therefore, <span class="math display">\[R=
\begin{bmatrix}
1-p &amp; 0 \\ 0 &amp; p
\end{bmatrix}
\text{ and }
Q=
\begin{bmatrix}
0 &amp; p \\ 1-p &amp; 0
\end{bmatrix}.\]</span> The matrix <span class="math inline">\(I-Q\)</span> is a <span class="math inline">\(2\times 2\)</span> matrix so it is easy
to invert: <span class="math display">\[(I-Q)^{-1}= \frac{1}{1-p+p^2}\begin{bmatrix}
1 &amp; p \\ 1-p &amp; 1
\end{bmatrix}.\]</span> So <span class="math display">\[U= \frac{1}{1-p+p^2}\begin{bmatrix}
1 &amp; p \\ 1-p &amp; 1
\end{bmatrix}
\begin{bmatrix}
1-p &amp; 0 \\ 0 &amp; p
\end{bmatrix}
=
\begin{bmatrix}
\frac{1-p}{1-p+p^2} &amp; \frac{p^2}{1-p+p^2} \\
\frac{(1-p)^2}{1-p+p^2} &amp; \frac{p}{1-p+p^2} \\
\end{bmatrix}.\]</span> Therefore, for the initial “wealth” is 1,
the probability of getting rich before bankruptcy is <span class="math inline">\(u_{13}=p^2/(1-p+p^2)\)</span> (the entry in the first row (<span class="math inline">\(x=1\)</span>) and the second column (<span class="math inline">\(a=3\)</span>) of <span class="math inline">\(U\)</span>.)</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-196" class="exercise"><strong>Problem 7.2  </strong></span>Find the probability of winning a whole game of Tennis, for a player whose
probability of winning a single rally is <span class="math inline">\(p=0.45\)</span>.</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-197" class="solution"><em>Solution</em>. </span>In the “Tennis” example, the transition matrix is <span class="math inline">\(20\times 20\)</span>, with only 2
recurrent states (each in its own class). In order to find the matrix <span class="math inline">\(U\)</span>, we (essentially) need to invert an <span class="math inline">\(18\times 18\)</span> matrix and that is a
job for a computer. We start with an R function which produces the transition
matrix <span class="math inline">\(P\)</span> as a function of the single-rally probability <span class="math inline">\(p\)</span>. Even though we only care about <span class="math inline">\(p=0.45\)</span> here, the extra flexibility will come in handy soon:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="absorption-and-reward.html#cb179-1" aria-hidden="true" tabindex="-1"></a>S<span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;0-0&quot;</span>, <span class="st">&quot;0-15&quot;</span>, <span class="st">&quot;15-0&quot;</span>, <span class="st">&quot;0-30&quot;</span>, <span class="st">&quot;15-15&quot;</span>, <span class="st">&quot;30-0&quot;</span>, <span class="st">&quot;0-40&quot;</span>, <span class="st">&quot;15-30&quot;</span>, </span>
<span id="cb179-2"><a href="absorption-and-reward.html#cb179-2" aria-hidden="true" tabindex="-1"></a>     <span class="st">&quot;30-15&quot;</span>, <span class="st">&quot;40-0&quot;</span>, <span class="st">&quot;15-40&quot;</span>, <span class="st">&quot;30-30&quot;</span>, <span class="st">&quot;40-15&quot;</span>, <span class="st">&quot;40-30&quot;</span>, <span class="st">&quot;30-40&quot;</span>, </span>
<span id="cb179-3"><a href="absorption-and-reward.html#cb179-3" aria-hidden="true" tabindex="-1"></a>     <span class="st">&quot;40-40&quot;</span>, <span class="st">&quot;40-A&quot;</span>, <span class="st">&quot;A-40&quot;</span>, <span class="st">&quot;P1&quot;</span>, <span class="st">&quot;P2&quot;</span>)</span>
<span id="cb179-4"><a href="absorption-and-reward.html#cb179-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb179-5"><a href="absorption-and-reward.html#cb179-5" aria-hidden="true" tabindex="-1"></a>tennis_P <span class="ot">=</span> <span class="cf">function</span>(p) {</span>
<span id="cb179-6"><a href="absorption-and-reward.html#cb179-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">c</span>( </span>
<span id="cb179-7"><a href="absorption-and-reward.html#cb179-7" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-8"><a href="absorption-and-reward.html#cb179-8" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-9"><a href="absorption-and-reward.html#cb179-9" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-10"><a href="absorption-and-reward.html#cb179-10" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-11"><a href="absorption-and-reward.html#cb179-11" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-12"><a href="absorption-and-reward.html#cb179-12" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-13"><a href="absorption-and-reward.html#cb179-13" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,</span>
<span id="cb179-14"><a href="absorption-and-reward.html#cb179-14" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-15"><a href="absorption-and-reward.html#cb179-15" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-16"><a href="absorption-and-reward.html#cb179-16" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,</span>
<span id="cb179-17"><a href="absorption-and-reward.html#cb179-17" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,</span>
<span id="cb179-18"><a href="absorption-and-reward.html#cb179-18" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">1</span><span class="sc">-</span>p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-19"><a href="absorption-and-reward.html#cb179-19" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,</span>
<span id="cb179-20"><a href="absorption-and-reward.html#cb179-20" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,</span>
<span id="cb179-21"><a href="absorption-and-reward.html#cb179-21" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,</span>
<span id="cb179-22"><a href="absorption-and-reward.html#cb179-22" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,p,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb179-23"><a href="absorption-and-reward.html#cb179-23" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,</span>
<span id="cb179-24"><a href="absorption-and-reward.html#cb179-24" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span><span class="sc">-</span>p,<span class="dv">0</span>,<span class="dv">0</span>,p,<span class="dv">0</span>,</span>
<span id="cb179-25"><a href="absorption-and-reward.html#cb179-25" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,</span>
<span id="cb179-26"><a href="absorption-and-reward.html#cb179-26" aria-hidden="true" tabindex="-1"></a>    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb179-27"><a href="absorption-and-reward.html#cb179-27" aria-hidden="true" tabindex="-1"></a> <span class="at">byrow=</span>T, <span class="at">ncol =</span> <span class="dv">20</span> )</span>
<span id="cb179-28"><a href="absorption-and-reward.html#cb179-28" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>The positions of the initial state “0-0” in the state-space vector
<code>S</code> is <span class="math inline">\(1\)</span>, and the
positions of the two absorbing states “P1” and “P2” are <span class="math inline">\(19\)</span> and <span class="math inline">\(20\)</span>.
Therefore the matrices <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> are obtained by vector indexing as follows:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="absorption-and-reward.html#cb180-1" aria-hidden="true" tabindex="-1"></a>P <span class="ot">=</span> <span class="fu">tennis_P</span>(<span class="fl">0.45</span>)</span>
<span id="cb180-2"><a href="absorption-and-reward.html#cb180-2" aria-hidden="true" tabindex="-1"></a>Q <span class="ot">=</span> P[<span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>]</span>
<span id="cb180-3"><a href="absorption-and-reward.html#cb180-3" aria-hidden="true" tabindex="-1"></a>R <span class="ot">=</span> P[<span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>, <span class="dv">19</span><span class="sc">:</span><span class="dv">20</span>]</span></code></pre></div>
<p>Linear systems are solved by using the command <code>solve</code> in R:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="absorption-and-reward.html#cb181-1" aria-hidden="true" tabindex="-1"></a>I <span class="ot">=</span> <span class="fu">diag</span>(<span class="dv">18</span>)  <span class="co"># the identity matrix the same size as Q</span></span>
<span id="cb181-2"><a href="absorption-and-reward.html#cb181-2" aria-hidden="true" tabindex="-1"></a>U <span class="ot">=</span> <span class="fu">solve</span>(I <span class="sc">-</span> Q, R)</span>
<span id="cb181-3"><a href="absorption-and-reward.html#cb181-3" aria-hidden="true" tabindex="-1"></a>U[<span class="dv">1</span>, ]</span>
<span id="cb181-4"><a href="absorption-and-reward.html#cb181-4" aria-hidden="true" tabindex="-1"></a><span class="do">## [1] 0.38 0.62</span></span></code></pre></div>
<p>Therefore, the probability that Player 1 wins the entire rally is about <span class="math inline">\(0.377\)</span>. Note
that this number is smaller than <span class="math inline">\(0.45\)</span>, so it appears that the game is
designed to make it easier for the better player to win. For more evidence,
let’s draw the graph of this probability for several values of <span class="math inline">\(p\)</span>
(<code>sapply</code> is the version of <code>apply</code> for vectors):</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="absorption-and-reward.html#cb182-1" aria-hidden="true" tabindex="-1"></a>prob_win <span class="ot">=</span> <span class="cf">function</span>(p) {</span>
<span id="cb182-2"><a href="absorption-and-reward.html#cb182-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (p <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb182-3"><a href="absorption-and-reward.html#cb182-3" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(p)</span>
<span id="cb182-4"><a href="absorption-and-reward.html#cb182-4" aria-hidden="true" tabindex="-1"></a>    P <span class="ot">=</span> <span class="fu">tennis_P</span>(p)</span>
<span id="cb182-5"><a href="absorption-and-reward.html#cb182-5" aria-hidden="true" tabindex="-1"></a>    Q <span class="ot">=</span> P[<span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>]</span>
<span id="cb182-6"><a href="absorption-and-reward.html#cb182-6" aria-hidden="true" tabindex="-1"></a>    R <span class="ot">=</span> P[<span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>, <span class="dv">19</span><span class="sc">:</span><span class="dv">20</span>]</span>
<span id="cb182-7"><a href="absorption-and-reward.html#cb182-7" aria-hidden="true" tabindex="-1"></a>    U <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">diag</span>(<span class="dv">18</span>) <span class="sc">-</span> Q, R)</span>
<span id="cb182-8"><a href="absorption-and-reward.html#cb182-8" aria-hidden="true" tabindex="-1"></a>    U[<span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb182-9"><a href="absorption-and-reward.html#cb182-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb182-10"><a href="absorption-and-reward.html#cb182-10" aria-hidden="true" tabindex="-1"></a>ps <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb182-11"><a href="absorption-and-reward.html#cb182-11" aria-hidden="true" tabindex="-1"></a>prob_game <span class="ot">=</span> <span class="fu">sapply</span>(ps, prob_win)</span></code></pre></div>
<p>A graph of <code>p</code> vs. <code>prob_game</code>, where the dashed line is the line <span class="math inline">\(y=x\)</span> looks like this:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-255-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Using a symbolic
software package (like <em>Mathematica</em>) we can even get an explicit
expression for the win probability in this case:
<span class="math display">\[\begin{align}
u_{(0,0)\ \   &quot;P1\ wins&quot;} = p^4 + 4 p^4 q + 10 p^4 q^2 + \frac{20 p^5 q^3}{1-2pq}.
\end{align}\]</span>
Actually, you don’t really need computers to derive the
expression above. Can you do it by finding all the ways in which the
game can be won in <span class="math inline">\(n=4,5,6,8, 10, 12, \dots\)</span> rallies, computing their
probabilities, and then adding them all up?</p>
</div>
</div>
<div id="expected-reward" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Expected reward<a href="absorption-and-reward.html#expected-reward" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose that each time you visit a transient state <span class="math inline">\(j\in T\)</span> you receive
a <em>reward</em> <span class="math inline">\(g(j)\in{\mathbb{R}}\)</span>. The name “reward” is a bit misleading since the
negative <span class="math inline">\(g(j)\)</span> corresponds more to a fine than to a reward; it is just
a name, anyway. Can we compute the expected total reward before
absorption <span class="math display">\[v_i={\mathbb{E}}_i[ \sum_{n=0}^{\tau_{C}-1} g(X_n)] ?\]</span> And if we
can, what is it good for? Many things, actually, as the following two
special cases show:</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(g(j)=1\)</span> for all <span class="math inline">\(j\in T\)</span>, then <span class="math inline">\(v_i\)</span> is the expected time until
absorption. We will calculate <span class="math inline">\(v_{(0,0)}\)</span> for the “Tennis” example
to compute the expected duration of a tennis game.</p></li>
<li><p>If <span class="math inline">\(g(k)=1\)</span> and <span class="math inline">\(g(j)=0\)</span> for <span class="math inline">\(j\not =k\)</span>, then <span class="math inline">\(v_i\)</span> is the expected
number of visits to the state <span class="math inline">\(k\)</span> before absorption. In the “Tennis”
example, if <span class="math inline">\(k=(40,40)\)</span>, the value of <span class="math inline">\(v_{(0,0)}\)</span> is the expected
number of times the score <span class="math inline">\((40,40)\)</span> is seen in a tennis game.</p></li>
</ol>
<p>We compute <span class="math inline">\(v_i\)</span> using the first-step decomposition: <span class="math display">\[\label{equ:}
   % \nonumber
   \begin{split}
v_i &amp;={\mathbb{E}}_i[ \sum_{n=0}^{\tau_C - 1} g(X_n)]
= g(i)+ {\mathbb{E}}_i[ \sum_{n=1}^{\tau_C - 1} g(X_n)]\\
&amp;= g(i)+ \sum_{k\in S} {\mathbb{E}}_i[ \sum_{n=1}^{\tau_C - 1} g(X_n)|X_1=k]
{\mathbb{P}}_i[X_1=k]\\
&amp;
= g(i)+ \sum_{k\in S}
p_{ik}{\mathbb{E}}_i[ \sum_{n=1}^{\tau_C - 1} g(X_n)|X_1=k]
   \end{split}\]</span> If <span class="math inline">\(k\in T\)</span>, then the Markov property implies that
<span class="math display">\[{\mathbb{E}}_i[ \sum_{n=1}^{\tau_C - 1} g(X_n)|X_1=k]={\mathbb{E}}_k[ \sum_{n=0}^{\tau_C - 1} g(X_n)]=v_k.\]</span>
When <span class="math inline">\(k\not\in T\)</span>, then
<span class="math display">\[{\mathbb{E}}_i[ \sum_{n=1}^{\tau_C - 1} g(X_n)|X_1=k]=0,\]</span> because we have
“arrived” and no more rewards are going to be collected. Therefore, for
<span class="math inline">\(i\in T\)</span> we have <span class="math display">\[v_i=g(i)+\sum_{k\in T} p_{ik} v_k.\]</span> If we organize
all <span class="math inline">\(v_i\)</span> and all <span class="math inline">\(g(i)\)</span> into column vectors <span class="math inline">\(v=(v_i, i\in T)\)</span>,
<span class="math inline">\(g=(g(i), i\in T)\)</span>, we get
<span class="math display">\[v=Qv+g, \text{ i.e., } v=(I-Q)^{-1} g = Fg.\]</span></p>
<p>Having derived the general formula for various rewards, we can provide
another angle to the interpretation of the fundamental matrix <span class="math inline">\(F\)</span>.
Let us pick a transient state <span class="math inline">\(j\)</span>
and use the reward function <span class="math inline">\(g\)</span> given by <span class="math display">\[g(k)=\mathbf{1}_{\{k=j\}}=
\begin{cases}
  1, &amp; k=j \\ 0,&amp; k\not= j.
\end{cases}\]</span> By the discussion above, the <span class="math inline">\(i^{th}\)</span> entry in
<span class="math inline">\(v=(I-Q)^{-1} g\)</span> is the expected reward when we start from the state
<span class="math inline">\(i\)</span>. Given the form of the reward function, <span class="math inline">\(v_i\)</span> is the expected number
of visits to the state <span class="math inline">\(j\)</span> when we start from <span class="math inline">\(i\)</span>. On the other hand, as
the product of the matrix <span class="math inline">\(F=(I-Q)^{-1}\)</span> and the vector
<span class="math inline">\(g=(0,0,\dots, 1, \dots, 0)\)</span>, <span class="math inline">\(v_i\)</span> is nothing but the <span class="math inline">\((i,j)\)</span>-entry in <span class="math inline">\(F=(I-Q)^{-1}\)</span>.</p>
<p>Let’s illustrate these ideas on some of our example chains:</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-198" class="exercise"><strong>Problem 7.3  </strong></span>What is the expected duration of a game of tennis? Compute it for
several values of the parameter <span class="math inline">\(p\)</span>.</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-199" class="solution"><em>Solution</em>. </span>The main idea is to perform a reward computation with <span class="math inline">\(g(i)=1\)</span> for all transient states <span class="math inline">\(i\)</span>. The R code is very similar to the one in the absorption example:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="absorption-and-reward.html#cb183-1" aria-hidden="true" tabindex="-1"></a>expected_duration <span class="ot">=</span> <span class="cf">function</span>(p) {</span>
<span id="cb183-2"><a href="absorption-and-reward.html#cb183-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (p <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb183-3"><a href="absorption-and-reward.html#cb183-3" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(<span class="dv">4</span>)</span>
<span id="cb183-4"><a href="absorption-and-reward.html#cb183-4" aria-hidden="true" tabindex="-1"></a>    P <span class="ot">=</span> <span class="fu">tennis_P</span>(p)</span>
<span id="cb183-5"><a href="absorption-and-reward.html#cb183-5" aria-hidden="true" tabindex="-1"></a>    Q <span class="ot">=</span> P[<span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">18</span>]</span>
<span id="cb183-6"><a href="absorption-and-reward.html#cb183-6" aria-hidden="true" tabindex="-1"></a>    g <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">1</span>, <span class="at">nrow =</span> <span class="dv">18</span>, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb183-7"><a href="absorption-and-reward.html#cb183-7" aria-hidden="true" tabindex="-1"></a>    v <span class="ot">=</span> <span class="fu">solve</span>(<span class="fu">diag</span>(<span class="dv">18</span>) <span class="sc">-</span> Q, g)</span>
<span id="cb183-8"><a href="absorption-and-reward.html#cb183-8" aria-hidden="true" tabindex="-1"></a>    v[<span class="dv">1</span>, ]</span>
<span id="cb183-9"><a href="absorption-and-reward.html#cb183-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb183-10"><a href="absorption-and-reward.html#cb183-10" aria-hidden="true" tabindex="-1"></a>ps <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb183-11"><a href="absorption-and-reward.html#cb183-11" aria-hidden="true" tabindex="-1"></a>duration_game <span class="ot">=</span> <span class="fu">sapply</span>(ps, expected_duration)</span></code></pre></div>
<p>As above, here is the graph of <code>p</code> vs. <code>duration_game</code>:
<img src="_main_files/figure-html/unnamed-chunk-257-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The maximum of the curve about equals to <span class="math inline">\(6.75\)</span>, and is achieved when the players are
evenly matched (<span class="math inline">\(p=0.5\)</span>).
Therefore, a game between fairly equally matched opponents lasts <span class="math inline">\(6.75\)</span>.
The game cannot be shorter than <span class="math inline">\(4\)</span> rallies and that is exactly the expected
duration when one player wins with certainty in each rally.</p>
</div>
<div class="exercise">
<p><span id="exr:unlabeled-div-200" class="exercise"><strong>Problem 7.4  </strong></span>What is the expected number of “deuces”, i.e., scores <span class="math inline">\((40,40)\)</span>? Compute it for
several values of the parameter <span class="math inline">\(p\)</span>.</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-201" class="solution"><em>Solution</em>. </span>This can be computed exactly as above, except that now the
reward function is given by
<span class="math display">\[\begin{align}
  g(i) = \begin{cases}
  1, &amp; \text{ if } i = (40,40),\\
  0, &amp; \text{ otherwise.}
  \end{cases}
\end{align}\]</span>
Since the code is almost identical to the code from the last example, we skip it
here and only draw the graph:
<img src="_main_files/figure-html/unnamed-chunk-258-1.png" width="672" style="display: block; margin: auto;" />
As expected, there are no dueces when <span class="math inline">\(p=0\)</span> or <span class="math inline">\(p=1\)</span>, and the
maximal expected number of dueces - <span class="math inline">\(0.625\)</span> - occurs when <span class="math inline">\(p=1/2\)</span>.</p>
<p>These numbers are a bit misleading, though, and, when asked,
people would usually give a higher
estimate for this expectation. The reason is that the expectation is a
poor summary of for the full distribution of the number of deuces.
The best way yo to get a feeling for the entire distribution is to
run some simulations. Here is the histogram of <span class="math inline">\(10000\)</span> simulations of a
game of tennis for the most interesting case <span class="math inline">\(p=0.5\)</span>:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-259-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We see that most of the games have no deuces. However, in the
cases where a deuce does happen, it is quite possible it will be repeated. A sizable number of draws yielded <span class="math inline">\(4\)</span> of more deuces.</p>
</div>
<p>We end with another example from a different area:</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-202" class="exercise"><strong>Problem 7.5  </strong></span>Alice plays the following game. She picks a pattern consisting of three
letters from the set <span class="math inline">\(\{H,T\}\)</span>, and then tosses a fair coin until her
pattern appears for the first time. If she has to pay <span class="math inline">\(\$1\)</span> for each
coin toss, what is the expected cost she is going to incur? What pattern
should she choose to minimize that cost?</p>
</div>
<div class="solution">
<p><span id="unlabeled-div-203" class="solution"><em>Solution</em>. </span>We start by choosing a pattern, say <span class="math inline">\(HTH\)</span>, and computing the number of
coin tosses Alice expects to make before it appears. This is just the
kind of computation that can be done using our absorption-and-reward
techniques, if we can find a suitable Markov chain. It turns out that
the following will do (green arrows stand for probability <span class="math inline">\(1/2\)</span>):</p>
<p><img src="pics/pattern_HTH_chain.png" style="display: block; margin: auto;" /></p>
<p>As Alice tosses the
coin, she keeps track of the largest initial portion of her pattern that
appears at last several places of the sequence of past tosses. The state
<span class="math inline">\(0\)</span> represents no such portion (as well as the intial state), while <span class="math inline">\(HT\)</span>
means that the last two coin tosses were <span class="math inline">\(H\)</span> and <span class="math inline">\(T\)</span> (in that order) so
that it is possible to end the game by tossing a <span class="math inline">\(H\)</span> next. On the other
hand, if the last toss was a <span class="math inline">\(T\)</span>, there is no need to keep track of that
- it is as good as <span class="math inline">\(0\)</span>.</p>
<p>Once we have this chain, all we have to do is perform the absorption and
reward computation with the reward function <span class="math inline">\(g\equiv 1\)</span>. The <span class="math inline">\(Q\)</span>-matrix
of this chain (with the transient states ordered as <span class="math inline">\(0, H, HT\)</span>) is
<span class="math display">\[Q = \begin{bmatrix}
1/2 &amp; 1/2 &amp; 0 \\
0 &amp; 1/2 &amp; 1/2 \\
1/2 &amp; 0 &amp;  0\\
\end{bmatrix}\]</span> and the fundamental matrix <span class="math inline">\(F\)</span> turns out to be <span class="math display">\[F =
\begin{bmatrix}
4 &amp; 4 &amp; 2 \\
2 &amp; 4 &amp; 2 \\
2 &amp; 2 &amp; 2 \\
\end{bmatrix} .\]</span> Therefore, the required expectation is the sum of all
the elements on the first row, i.e., <span class="math inline">\(10\)</span>.</p>
<p>Let us repeat the same for the pattern <span class="math inline">\(HHH\)</span>. We build a similar Markov
chain:</p>
<p><img src="pics/pattern_HHH_chain.png" style="display: block; margin: auto;" /></p>
<p>We see that
there is a subtle difference. One transition from the state <span class="math inline">\(H\)</span>, instead
of going back to itself, is directed towards <span class="math inline">\(0\)</span>. It is clear from here,
that this can only increase Alice’s cost. Indeed, the fundamental matrix
is now given by <span class="math display">\[F =
\begin{bmatrix}
8 &amp; 4 &amp; 2 \\
6 &amp; 4 &amp; 2 \\
4 &amp; 2 &amp; 2 \\
\end{bmatrix},\]</span> and the expected number of tosses before the first
appearance of <span class="math inline">\(HHH\)</span> comes out as <span class="math inline">\(14\)</span>.</p>
<p>Can you do this for other patterns? Which one should Alice choose to
minimize her cost?</p>
</div>
</div>
<div id="additional-problems-for-chapter-7" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Additional Problems for Chapter 7<a href="absorption-and-reward.html#additional-problems-for-chapter-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div style="border: 1px solid; padding: 5px;">
<p><strong>Note:</strong> do not use simulations in any of the problems below. Using R (or other software) to manipulate matrices or perform other numerical computation is fine.</p>
</div>
<p><br></p>
<!--
  mc_prob1
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-204" class="exercise"><strong>Problem 7.6  </strong></span>The fundamental matrix associated to a finite Markov chain is
<span class="math inline">\(F = \begin{bmatrix} 3 &amp; 3 \\ 3/2 &amp; 3\end{bmatrix}\)</span>, with the first row (and column)
corresponding to the state <span class="math inline">\(A\)</span> and the second to <span class="math inline">\(B\)</span>. Some of the
following statements are true and the others are false. Find which ones
are true and which are false; give explanations for your choices.</p>
<ol style="list-style-type: decimal">
<li><p>The chain has <span class="math inline">\(2\)</span> recurrent states.</p></li>
<li><p>If the chain starts in <span class="math inline">\(A\)</span>, the expected number of visits to <span class="math inline">\(B\)</span>
before hitting the first recurrent state is <span class="math inline">\(3\)</span>.</p></li>
<li><p>If the chain is equally likely to start from <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>, the
expected number of steps it will take before it hits its first
recurrent state is <span class="math inline">\(\frac{21}{4}\)</span>.</p></li>
<li><p><span class="math inline">\({\mathbb{P}}_A[X_1=C] =0\)</span> for any recurrent state <span class="math inline">\(C\)</span>.</p></li>
</ol>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  \ -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/mc_prob1_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  ar-prob-01
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-205" class="exercise"><strong>Problem 7.7  </strong></span>In a Markov chain with a finite number of states, the fundamental matrix
is given by <span class="math display">\[F=\begin{bmatrix} 3 &amp; 4 \\
    \tfrac{3}{2} &amp; 4\end{bmatrix}.\]</span> The initial distribution of the
chain is uniform on all transient states. Compute the expected value of
<span class="math display">\[\tau_C=\min \{ n\in{\mathbb{N}}_0\, : \, X_n\in C\},\]</span> where <span class="math inline">\(C\)</span> denotes the set of
all recurrent states.</p>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/ar-prob-01_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  ar-prob-03
  ------------------------------------------------
-->
<div id="Gambler">
<div class="exercise">
<p><span id="exr:unlabeled-div-206" class="exercise"><strong>Problem 7.8  </strong></span>Consider the “Gambler’s ruin” model with parameter <span class="math inline">\(p\)</span>.
Write an R function that computes the
(vector of) probabilities that the gambler will go bankrupt before her wealth reaches
<span class="math inline">\(\$1000\)</span> for each initial wealth <span class="math inline">\(x = 0,1,\dots, 1000\)</span>.
Plot the graphs for <span class="math inline">\(p=0.4, p=0.49, p=0.499\)</span> and <span class="math inline">\(p=0.5\)</span> on top of each other. Did you expect them to look the way they do?</p>
</div>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/ar-prob-03_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
 bold-play 
  ------------------------------------------------
-->
<!--::: {.exercise} -->
<!-- ```{r child="problems/04_Absorption_and_Reward/bold-play_prb.Rmd"} -->
<!-- ``` -->
<!--::: -->
<!-- <!--  <details>  -->
<p>–&gt;
<!-- <!--  <summary> Click for Solution </summary>  --> –&gt;
<!-- <!-- ::: {.solution}  --> –&gt;
<!--  -->
<!-- <!--  ```{r child="problems/04_Absorption_and_Reward/bold-play_sol.Rmd"}  --> –&gt;
<!-- <!--  ```  --> –&gt;
<!--  -->
<!-- <!-- :::  --> –&gt;
<!-- <!--  </details>  --> –&gt;</p>
<!--
  mc_prob2
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-207" class="exercise"><strong>Problem 7.9  </strong></span>A basketball player is shooting a series of free throws. The probability
of hitting any single one is <span class="math inline">\(1/2\)</span>, and the throws are independent of
each other. What is the expected number of throws the player will
attempt before hitting 3 free throws in a row (including those 3)?</p>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/mc_prob2_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  ar-prob-04
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-208" class="exercise"><strong>Problem 7.10  </strong></span>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a Markov chain
with the following transition matrix <span class="math display">\[P=
\begin{bmatrix}
  1/2 &amp; 1/2 &amp; 0 \\
  1/3 &amp; 1/3 &amp; 1/3 \\
  0  &amp;  0  &amp;  1\\
\end{bmatrix}\]</span> Suppose that the chain starts from the state <span class="math inline">\(1\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>What is expected time that will pass before the chain first hits
<span class="math inline">\(3\)</span>?</p></li>
<li><p>What is the expected number of visits to state <span class="math inline">\(2\)</span> before <span class="math inline">\(3\)</span> is
hit?</p></li>
<li><p>Would your answers to 1. and 2. change if we replaced values in the
third row of <span class="math inline">\(P\)</span> by any other values (as long as <span class="math inline">\(P\)</span> remains a
stochastic matrix)? Would <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> still be transient states?</p></li>
<li><p>Use the idea of part 3. to answer the following question. What is
the expected number of visits to the state <span class="math inline">\(2\)</span> before a Markov chain
with transition matrix <span class="math display">\[P=
\begin{bmatrix}
17/20 &amp; 1/20 &amp; 1/10\\
1/15 &amp; 13/15 &amp; 1/15\\
2/5 &amp; 4/15 &amp; 1/3\\
  \end{bmatrix}\]</span> hits the state <span class="math inline">\(3\)</span> for the first time (the initial
state is still <span class="math inline">\(1\)</span>)? Remember this trick for your next exam.</p></li>
</ol>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/ar-prob-04_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  ar-prob-02
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-209" class="exercise"><strong>Problem 7.11  </strong></span>A fair 6-sided die is rolled repeatedly, and for <span class="math inline">\(n\in{\mathbb{N}}\)</span>, the outcome
of the <span class="math inline">\(n\)</span>-th roll is denoted by <span class="math inline">\(Y_n\)</span> (it is assumed that <span class="math inline">\(\{Y_n\}_{n\in{\mathbb{N}}}\)</span> are
independent of each other). For <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>, let <span class="math inline">\(X_n\)</span> be the remainder
(taken in the set <span class="math inline">\(\{0,1,2,3,4\}\)</span>) left after the sum
<span class="math inline">\(\sum_{k=1}^n Y_k\)</span> is divided by <span class="math inline">\(5\)</span>, i.e. <span class="math inline">\(X_0=0\)</span>, and <span class="math display">\[%\label{}
    \nonumber
    \begin{split}
X_n= \sum_{k=1}^n Y_k \ (\,\mathrm{mod}\, 5\,),\text{ for } n\in{\mathbb{N}},
    \end{split}\]</span> making <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> a Markov chain on the state space
<span class="math inline">\(\{0,1,2,3,4\}\)</span> (no need to prove this fact).</p>
<ol style="list-style-type: decimal">
<li><p>Write down the transition matrix of the chain.</p></li>
<li><p>Classify the states, separate recurrent from transient ones, and
compute the period of each state.</p></li>
<li><p>Compute the expected number of rolls before the first time
<span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> visits the state <span class="math inline">\(2\)</span>, i.e., compute <span class="math inline">\({\mathbb{E}}[\tau_2]\)</span>, where
<span class="math display">\[\tau_2=\min \{ n\in{\mathbb{N}}_0\, : \, X_n=2\}.\]</span></p></li>
<li><p>Compute <span class="math inline">\({\mathbb{E}}[\sum_{k=0}^{\tau_2-1} X_k]\)</span>.</p></li>
</ol>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  \ -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/ar-prob-02_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  ar-prob-15
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-210" class="exercise"><strong>Problem 7.12  </strong></span>Let <span class="math inline">\(\{Y_n\}_{n\in {\mathbb{N}}_0}\)</span> be a sequence
of die-rolls, i.e., a sequence of independent random variables with
distribution <span class="math display">\[Y_n \sim \left(
\begin{array}{cccccc}
1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 \\
1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6
  \end{array}
\right).\]</span> Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a stochastic process defined by
<span class="math inline">\(X_n=\max (Y_0,Y_1, \dots, Y_n)\)</span>. In words, <span class="math inline">\(X_n\)</span> is the maximal value rolled so far.</p>
<ol style="list-style-type: decimal">
<li><p>Explain why <span class="math inline">\(X\)</span> is a Markov chain, and find its transition matrix
and the initial distribution.</p></li>
<li><p>Supposing that the first roll of the die was <span class="math inline">\(3\)</span>, i.e., <span class="math inline">\(X_0=3\)</span>,
what is the expected time until a <span class="math inline">\(6\)</span> is reached?</p></li>
<li><p>Under the same assumption as above (<span class="math inline">\(X_0=3\)</span>), what is the
probability that a <span class="math inline">\(5\)</span> will not be rolled before a <span class="math inline">\(6\)</span> is rolled for
the first time?</p></li>
<li><p>Starting with the first value <span class="math inline">\(X_0=3\)</span>, each time a die is rolled,
the current record (the value of <span class="math inline">\(X_n\)</span>) is written down. When a <span class="math inline">\(6\)</span>
is rolled for the first time all the numbers are added up and the
result is called <span class="math inline">\(S\)</span> (the final <span class="math inline">\(6\)</span> is not counted). What is the
expected value of <span class="math inline">\(S\)</span>?</p></li>
</ol>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  \ -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/ar-prob-15_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  basil-comp
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-211" class="exercise"><strong>Problem 7.13  </strong></span>Go back to the problem with
<a href="Markov-chains.html#basil">Basil the rat</a> in the he first lecture on Markov chains and answer the question 2., but this time using an absorption/reward computation.</p>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/basil-comp_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->
<!--
  prof-comp
  ------------------------------------------------
-->
<div class="exercise">
<p><span id="exr:unlabeled-div-212" class="exercise"><strong>Problem 7.14  </strong></span>Go back to the problem with the
<a href="Markov-chains.html#professor">professor and his umbrellas</a> in the first lecture on Markov chains and answer the questions in part 2., but this time using an absorption/reward computation.</p>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!--  -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/prof-comp_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  -->
<!--  </details> -->
<!--
  facility
  ------------------------------------------------
-->
<div id="facility">

<div class="exercise">
<p><span id="exr:unlabeled-div-213" class="exercise"><strong>Problem 7.15  </strong></span>An airline reservation system has two computers. Any computer in operation may break down on any given day with probability <span class="math inline">\(p=0.3\)</span>, independently of the other computer.
There is a single repair facility which takes two days to restore a
computer to normal. It can work on only one computer at a time, and if two computers need work at the same time, one of them has to wait and enters the facility as soon as it is free again.</p>
<p>The system starts with one operational computer; the other one broke last night and just entered the repair facility this morning.</p>
<ol style="list-style-type: decimal">
<li><p>Compute the probability that at no time will both computers be down simultaneously between now and the first time both computers are operational.</p></li>
<li><p>Assuming that each day with only one working computer costs the company <span class="math inline">\(\$10,000\)</span> and each day with both computers down <span class="math inline">\(\$30,000\)</span>, what is the total cost the company is expected to incur between now and the first time both computers are operational again.</p></li>
</ol>
</div>
<!--  <details> -->
<!--  <summary> Click for Solution </summary> -->
<!--  -->
<!-- ::: {.solution} -->
<!--  -->
<!--  ```{r child="problems/04_Absorption_and_Reward/facility_sol.Rmd"} -->
<!--  ``` -->
<!--  -->
<!-- ::: -->
<!--  </details> -->

</div>
</div>

































            </section>

          </div>
        </div>
      </div>
<a href="classification-of-states.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": false,
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/gordanz/M362M/blob/master/source/07-Absorption-and-Reward.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toc_depth": null
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
